#!/usr/bin/env python3
"""
Bullinger MWE with Qwen3-VL:
- Walk datasets/bullinger_handwritten/gt/*.txt to get IDs
- Collect images in datasets/bullinger_handwritten/images/<ID>/** (supports multi-page)
- Transcribe with Qwen/Qwen3-VL-8B-Instruct (vision-language)
- Write predictions/<ID>.txt
- Compute WER/CER and line-level accuracy (forward + reverse, raw + normalized)
  -> evaluation_qwen.csv

"""

import argparse
import csv
import glob
import os
import sys
from dataclasses import dataclass
from typing import List

from PIL import Image
from metrics import (
    wer,
    cer,
    normalize_whitespace,
    line_accuracy,
    line_accuracy_norm,
    reverse_line_accuracy,
    reverse_line_accuracy_norm,
)

# ---------------- Data helpers ----------------
IMG_EXTS = (".jpg", ".jpeg", ".png", ".tif", ".tiff")

def find_images_for_id(images_root: str, sample_id: str) -> List[str]:
    base = os.path.join(images_root, sample_id)
    if not os.path.isdir(base): return []
    cand = []
    # direct files
    for ext in IMG_EXTS:
        cand += sorted(glob.glob(os.path.join(base, f"*{ext}")))
    # common subfolders
    for sub in ("page", "images", "img"):
        d = os.path.join(base, sub)
        if os.path.isdir(d):
            for ext in IMG_EXTS:
                cand += sorted(glob.glob(os.path.join(d, f"*{ext}")))
    # last resort: recursive
    if not cand:
        for ext in IMG_EXTS:
            cand += sorted(glob.glob(os.path.join(base, "**", f"*{ext}"), recursive=True))
    # dedup keeping order
    seen, out = set(), []
    for p in cand:
        if p not in seen:
            seen.add(p); out.append(p)
    return out

def read_text(p: str) -> str:
    with open(p, "r", encoding="utf-8") as f:
        return f.read().strip()

def write_text(p: str, t: str):
    os.makedirs(os.path.dirname(p), exist_ok=True)
    with open(p, "w", encoding="utf-8") as f:
        f.write(t)

# ---------------- Qwen backend ----------------
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq

@dataclass
class QwenCfg:
    model_id: str = "Qwen/Qwen3-VL-8B-Instruct"
    device: str = "auto"   # "auto" | "cuda" | "cpu"
    max_new_tokens: int = 800

class QwenTranscriber:
    def __init__(self, cfg: QwenCfg):
        self.device = "cuda" if (cfg.device in ("auto", "cuda") and torch.cuda.is_available()) else "cpu"
        self.processor = AutoProcessor.from_pretrained(cfg.model_id, trust_remote_code=True)

        load_kwargs = dict(trust_remote_code=True)
        if self.device == "cuda":
            # Prefer 4-bit quantization to fit on 32GB V100
            try:
                load_kwargs.update({
                    "device_map": "auto",
                    "load_in_4bit": True,
                    "bnb_4bit_compute_dtype": torch.float16,
                    "bnb_4bit_quant_type": "nf4",
                    "bnb_4bit_use_double_quant": True,
                })
            except Exception:
                # Fallback to fp16 if bitsandbytes not available
                load_kwargs.update({
                    "device_map": "auto",
                    "torch_dtype": torch.float16,
                })

        self.model = AutoModelForVision2Seq.from_pretrained(cfg.model_id, **load_kwargs)
        self.model.eval()
        self.max_new_tokens = cfg.max_new_tokens

    def _prompt(self) -> str:
        return (
            "Transcribe this handwritten page to plain text. "
            "Output ONLY the transcription. Preserve line breaks. No extra words."
        )

    def _downscale(self, img: Image.Image, max_side: int = 1280) -> Image.Image:
        w, h = img.size
        s = max(w, h)
        if s <= max_side:
            return img
        scale = max_side / float(s)
        return img.resize((int(w * scale), int(h * scale)))

    @torch.inference_mode()
    def _generate_one(self, img: Image.Image) -> str:
        # Build chat message so image tokens align with features
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": img},
                {"type": "text",  "text": self._prompt()},
            ],
        }]
        text = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        inputs = self.processor(text=[text], images=[img], return_tensors="pt")
        if self.device == "cuda":
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        out_ids = self.model.generate(
            **inputs,
            max_new_tokens=self.max_new_tokens,
            do_sample=False,
            temperature=0.0,
            num_beams=1,
            repetition_penalty=1.05,
        )
        return self.processor.batch_decode(out_ids, skip_special_tokens=True)[0].strip()

    def transcribe_images(self, image_paths: List[str]) -> str:
        texts = []
        for p in image_paths:
            img = Image.open(p).convert("RGB")
            img = self._downscale(img, max_side=1280)
            txt = self._generate_one(img)
            texts.append(txt)
            if self.device == "cuda":
                torch.cuda.empty_cache()
        return "\n".join(texts).strip()

# ---------------- Main ----------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data-dir", default="datasets/bullinger_handwritten", help="folder containing gt/ and images/")
    ap.add_argument("--out-dir", default="predictions", help="where to write predictions")
    ap.add_argument("--eval-csv", default="evaluation_qwen.csv", help="output CSV path")
    ap.add_argument("--hf-model", default="Qwen/Qwen3-VL-8B-Instruct")
    ap.add_argument("--hf-device", default="auto", choices=["auto", "cuda", "cpu"])
    ap.add_argument("--max-new-tokens", type=int, default=2048)
    args = ap.parse_args()

    # Instantiate backend
    transcriber = QwenTranscriber(QwenCfg(
        model_id=args.hf_model,
        device=args.hf_device,
        max_new_tokens=args.max_new_tokens,
    ))

    gt_dir = os.path.join(args.data_dir, "gt")
    images_root = os.path.join(args.data_dir, "images")
    gt_files = sorted(glob.glob(os.path.join(gt_dir, "*.txt")))
    if not gt_files:
        print(f"No ground-truth files found in {gt_dir}", file=sys.stderr)
        sys.exit(1)

    rows = []
    n = 0
    sum_w = sum_c = sum_wn = sum_cn = 0.0
    sum_la = sum_lan = sum_rla = sum_rlan = 0.0

    for gt_path in gt_files:
        sample_id = os.path.splitext(os.path.basename(gt_path))[0]
        img_paths = find_images_for_id(images_root, sample_id)
        if not img_paths:
            print(f"[WARN] No images for {sample_id}; skipping.", file=sys.stderr)
            continue

        pred = transcriber.transcribe_images(img_paths)
        write_text(os.path.join(args.out_dir, f"{sample_id}.txt"), pred)

        gt = read_text(gt_path)
        w, c = wer(gt, pred), cer(gt, pred)
        wn, cn = wer(normalize_whitespace(gt), normalize_whitespace(pred)), cer(normalize_whitespace(gt), normalize_whitespace(pred))
        la  = line_accuracy(gt, pred)
        lan = line_accuracy_norm(gt, pred)
        rla = reverse_line_accuracy(gt, pred)
        rlan = reverse_line_accuracy_norm(gt, pred)

        rows.append([sample_id, len(gt), len(pred), w, c, wn, cn, la, lan, rla, rlan])
        sum_w += w; sum_c += c; sum_wn += wn; sum_cn += cn
        sum_la += la; sum_lan += lan; sum_rla += rla; sum_rlan += rlan
        n += 1
        print(
            f"[OK] {sample_id}: "
            f"WER={w:.3f} CER={c:.3f} "
            f"(norm WER={wn:.3f} CER={cn:.3f}) "
            f"LineAcc={la:.3f} LineAcc_norm={lan:.3f} "
            f"RevLineAcc={rla:.3f} RevLineAcc_norm={rlan:.3f}"
        )

    # Write CSV (+ macro average)
    os.makedirs(os.path.dirname(args.eval_csv) or ".", exist_ok=True)
    with open(args.eval_csv, "w", newline="", encoding="utf-8") as f:
        wtr = csv.writer(f)
        wtr.writerow([
            "id",
            "len_gt",
            "len_pred",
            "wer",
            "cer",
            "wer_norm",
            "cer_norm",
            "line_acc",
            "line_acc_norm",
            "rev_line_acc",
            "rev_line_acc_norm",
        ])
        wtr.writerows(rows)
        if n > 0:
            wtr.writerow([])
            wtr.writerow([
                "macro_avg",
                "",
                "",
                sum_w/n,
                sum_c/n,
                sum_wn/n,
                sum_cn/n,
                sum_la/n,
                sum_lan/n,
                sum_rla/n,
                sum_rlan/n,
            ])

    print(f"\nWrote {args.eval_csv} with {n} samples.")

if __name__ == "__main__":
    main()
